{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BQML - LinRegression with Vizier HyperParam-Tuning + XAI on VertexAI Pipelines\n",
    "**Training pipeline for BQML model using Vizier HyperParameter tuning for optimal model selection and explainability AI (XAI)**\n",
    "\n",
    "The following creates a pipeline which trains a BigQuery Model using the hyperparameter tuning feature, evaluates and selects the optimal model and then deploys it to an end point if it meets the minimum threshold performance. The model is trained with the new XAI feature allowing us to obtain explainabilty at both global and prediction level. \n",
    "\n",
    "\n",
    "**Objective** \n",
    "\n",
    "Predict the tip amount for a NewYork taxi ride using a linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bqPipeline_small.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Create or use a [Google Cloud Storage](https://console.cloud.google.com/storage) bucket to export the model to. <strong>Make sure to create the bucket in the same region where you will create Vertex AI Endpoint to host your model.</strong> \n",
    "* Create a BigQuery dataset to create the model in. <strong>Make sure to create the dataset in the same region where you will create Vertex AI Endpoint and bucket.</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push a docker image using Dockerfile as the base image for the Kubeflow pipeline components\n",
    "!./dockerbuild.sh\n",
    "# if Permission error run: chmod +x dockerbuild.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the KFP version, The KFP version should be >= 1.6. If lower, run !pip3 install --user kfp --upgrade, then restart the kernel\n",
    "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# CHANGE the following settings\n",
    "PROJECT_ID=\"sandbox-marina\" #This is your GCP project ID\n",
    "REGION='us-central1' # Vertex AI endpoint deployment region must match bucket region\n",
    "USER = 'marinadeletic'\n",
    "BUCKET_NAME = 'sandbox_marina_vertex' \n",
    "BQ_DATASET_NAME=\"taxi\" #This is the name of the target dataset where you model and predictions will be stored\n",
    "MODEL_NAME='hp_xai_taxi_tip_model'\n",
    "\n",
    "# Required Parameters for Vertex AI\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER) #Cloud Storage URI that your pipelines service account can access.\n",
    "BASE_IMAGE='gcr.io/{}/bq_vertexai_container:latest'.format(PROJECT_ID)  #This is the image built from the Dockfile in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to \n",
    "\n",
    "MODEL_QUERY = f\"\"\"\n",
    "CREATE or REPLACE MODEL {BQ_DATASET_NAME}.{MODEL_NAME}\n",
    "OPTIONS\n",
    "  (model_type='linear_reg',\n",
    "    num_trials=10,\n",
    "    max_parallel_trials=2,\n",
    "    enable_global_explain = TRUE) AS\n",
    "SELECT\n",
    "  * \n",
    "FROM(\n",
    "    SELECT\n",
    " * EXCEPT(tip_amount),\n",
    " tip_amount AS label\n",
    "FROM\n",
    " `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018`\n",
    "WHERE\n",
    " tip_amount IS NOT NULL\n",
    "LIMIT\n",
    " 100000\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "EVAL_QUERY= f\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{MODEL_NAME})\n",
    "ORDER BY  r2_score desc \n",
    "LIMIT 1\"\"\"\n",
    "\n",
    "# Specifiy the Named Tuple output format of the evaluation query based on the output format of the EVAL_QUERY. \n",
    "# i.e the above EVAL_QUERY results in a table with a number of evaluation metrics: \n",
    "# ['trial_id', 'mean_absolute_error'... 'r2_score'], for the output of the evaluation component we only want to display the trial_id and r2_score\n",
    "# so here we define the output tuple format.\n",
    "\n",
    "EVAL_OUTPUT_TUPLE = [(\"trial_id\", int), (\"r2_score\", float)] \n",
    "\n",
    "EVAL_THRESHOLD = (\"r2_score\", 0.5) # specify the metric and the threshold value required to deploy the model. Ensure this metric is defined in EVAL_OUTPUT_TUPLE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Component Functions\n",
    "\n",
    "* **BigQuery function**.-  A generic BigQuery function that runs a BigQuery query and returns the table/model created. This will be re-used to return BigQuery results for all the different segments of the BigQuery process in the Kubeflow Pipeline. You will see later in the tutorial where this function is being passed as parameter (ddlop) to other functions to perform certain BigQuery operation.\n",
    "\n",
    "* **Training helper function**\n",
    "* **Evaluate function** - Outputs the EVAL_QUERY in the format of the EVAL_OUTPUT_TUPLE \n",
    "* **Export Model Function** - Exports the BQML model to GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "import json\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "def run_bigquery(project_id: str, query_string: str, location: str) -> NamedTuple(\n",
    "    'DDLOutput', [('created_asset', str), ('query', str)]):\n",
    "    \"\"\"\n",
    "    Runs BigQuery query and returns a table/model name\n",
    "    \"\"\"\n",
    "    print(query_string)\n",
    "        \n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.future import polling\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.bigquery import retry as bq_retry\n",
    "    \n",
    "    bqclient = bigquery.Client(project=project_id, location=location)\n",
    "    job = bqclient.query(query_string, retry=bq_retry.DEFAULT_RETRY)\n",
    "    job._retry = polling.DEFAULT_RETRY\n",
    "    \n",
    "    while job.running():\n",
    "        from time import sleep\n",
    "        sleep(0.1)\n",
    "        print('Running ...')\n",
    "        \n",
    "    tblname = job.ddl_target_table\n",
    "    tblname = '{}.{}'.format(tblname.dataset_id, tblname.table_id)\n",
    "    print('{} created in {}'.format(tblname, job.ended - job.started))\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result_tuple = namedtuple('DDLOutput', ['created_asset', 'query'])\n",
    "    return result_tuple(tblname, query_string)\n",
    "\n",
    "def train_bq_model(ddlop, project_id, model_query):\n",
    "    query = model_query\n",
    "    print(query)\n",
    "    return ddlop(project_id, query, 'US')\n",
    "\n",
    "def evaluate(project_id: str, model_name: str, eval_query: str, target_metrics: str) -> NamedTuple(\"Outputs\", EVAL_OUTPUT_TUPLE):\n",
    "    query = eval_query\n",
    "    print(query)\n",
    "    from google.cloud import bigquery\n",
    "    bqclient = bigquery.Client(project=project_id, location='US')\n",
    "    results = bqclient.query(query).result().to_dataframe()\n",
    "    \n",
    "    target_metrics= list(target_metrics.split(\",\")) # converts target metrics back to list\n",
    "    keyresults= results[target_metrics]\n",
    "    return list(keyresults.itertuples(name='Outputs', index=False))[0]\n",
    "\n",
    "def export_bqml_model(project_id:str, model:str, bucket_name:str) -> NamedTuple('ModelExport', [('model_name', str),('destination', str)]):\n",
    "    import subprocess\n",
    "    import shutil\n",
    "    #bq extract -m {PROJECT_ID}:{DATASET_NAME}.{MODEL_NAME} gs://{BUCKET_NAME}/{MODEL_NAME}\n",
    "    model_name = '{}:{}'.format(project_id, model)\n",
    "    destination = 'gs://{}/{}'.format(bucket_name, model)\n",
    "    print (model_name)\n",
    "\n",
    "    subprocess.run(\n",
    "        (\n",
    "            shutil.which(\"bq\"),\n",
    "            \"extract\",\n",
    "            \"--project_id=\" + project_id,\n",
    "            \"-m\",\n",
    "            model_name,\n",
    "            destination\n",
    "        ),\n",
    "        stderr=subprocess.PIPE,\n",
    "        check=True)\n",
    "    return (model, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.v2.dsl as dsl\n",
    "import kfp.components as comp\n",
    "import time\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"bq-vizier-xai-pipeline\",\n",
    "    description='training pipeline for BQML model using Vizier HyperParameter tuning for optimal model selection')\n",
    "\n",
    "def training_pipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    model_query: str= MODEL_QUERY,\n",
    "    eval_query: str= EVAL_QUERY,\n",
    "    bucket_name: str= BUCKET_NAME,\n",
    "    model_dispay_name: str = f'{MODEL_NAME}_vai',\n",
    "    eval_threshold_metric: str = EVAL_THRESHOLD[0],\n",
    "    eval_threshold_val: float = EVAL_THRESHOLD[1]):\n",
    "    \n",
    "    ddlop = comp.func_to_container_op(run_bigquery, packages_to_install=['google-cloud-bigquery'])\n",
    "        \n",
    "    #Create Model\n",
    "    bq_model_output = train_bq_model(ddlop, project_id, model_query).set_display_name('create BQ model')\n",
    "    bq_model_output.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    bq_model_dest = bq_model_output.outputs['created_asset']\n",
    "    \n",
    "    #Evaluate Model\n",
    "    evalop = comp.func_to_container_op(evaluate, packages_to_install=['google-cloud-bigquery', 'pandas','pyarrow'])\n",
    "    target_metrics=[i[0] for i in EVAL_OUTPUT_TUPLE] # extracts key metrics as list\n",
    "    target_metrics = ','.join(map(str, target_metrics)) # converts key metrics to string for kfp component to ingest\n",
    "    error = evalop(PROJECT_ID, bq_model_dest, eval_query, target_metrics)\n",
    "    \n",
    "    with dsl.Condition(error.outputs[\"r2_score\"] > eval_threshold_val, name=\"deploy_decision\"):\n",
    "        #Export Model\n",
    "        export_bqml_model_op = comp.func_to_container_op(export_bqml_model, base_image=BASE_IMAGE, output_component_file='export_bqml.yaml')   \n",
    "        export_destination_output = export_bqml_model_op(project_id, bq_model_dest, bucket_name).set_display_name('export BQ model')\n",
    "        export_destination_output.execution_options.caching_strategy.max_cache_staleness = 'P0D' \n",
    "        export_destination = export_destination_output.outputs['destination']\n",
    "\n",
    "        #Upload Model \n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=model_dispay_name,\n",
    "            artifact_uri=export_destination,\n",
    "            serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest',\n",
    "        )\n",
    "        #Deploy Model\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "            project=project_id,\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=model_dispay_name,\n",
    "            machine_type=\"n1-standard-4\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{pipelineparam:op=;name=model_query}}\n"
     ]
    }
   ],
   "source": [
    "import kfp.v2 as kfp\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "pipeline_func = training_pipeline\n",
    "compiler.Compiler().compile(pipeline_func=pipeline_func, \n",
    "                            package_path='bq_pipeline_job.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "This will launch the pipeline to run in Vertex AI Pipelines. It will take ~30min to complete the training. The Training of the BQML model  can be seen in the BQ console, selecting the training view of the model will illustrate each itteration of the model and the relevant metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v1.9. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bq-vizier-xai-pipeline-20210915110744?project=sandbox-marina\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path='bq_pipeline_job.json', \n",
    "    enable_caching=False,\n",
    "    pipeline_root=PIPELINE_ROOT \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BQ console link to model\n",
    "f'https://console.cloud.google.com/bigquery?p={PROJECT_ID}&d={BQ_DATASET_NAME}&page=model&m={MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability AI \n",
    "As we included the option `enable_global_explain` in the model creation statement we will be able to run analysis of the explainability of the model using a simple QUERY. We use`ML.GLOBAL_EXPLAIN` with a linear_reg model. See [Documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview#explainable_ai_offerings_in_bigquery_ml) for other model type syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  attribution\n",
      "0          payment_type     2.339830\n",
      "1          total_amount     1.110914\n",
      "2    pickup_location_id     0.776240\n",
      "3   dropoff_location_id     0.761665\n",
      "4    store_and_fwd_flag     0.719575\n",
      "5             vendor_id     0.718385\n",
      "6             rate_code     0.716977\n",
      "7          tolls_amount     0.145415\n",
      "8           fare_amount     0.070397\n",
      "9               mta_tax     0.026552\n",
      "10        imp_surcharge     0.017515\n",
      "11                extra     0.014638\n",
      "12      passenger_count     0.012904\n",
      "13        trip_distance     0.012815\n",
      "14     dropoff_datetime     0.000241\n",
      "15      pickup_datetime     0.000024\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "EXPLAIN_QUERY=f'SELECT * FROM ML.GLOBAL_EXPLAIN(MODEL {BQ_DATASET_NAME}.{MODEL_NAME})'\n",
    "\n",
    "client = bigquery.Client()\n",
    "query_job = client.query(EXPLAIN_QUERY)\n",
    "print(query_job.result().to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict at EndPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxi-pred.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxi-pred.json\n",
    "{\"instances\":[{\n",
    "    \"vendor_id\": \"2\",\n",
    "    \"pickup_datetime\": \"2018-05-15T11:15:12\",\n",
    "    \"dropoff_datetime\": \"2018-05-15T12:13:40\",\n",
    "    \"passenger_count\": 1,\n",
    "    \"trip_distance\": 11.1,\n",
    "    \"rate_code\": \"1\",\n",
    "    \"store_and_fwd_flag\": \"N\",\n",
    "    \"payment_type\": \"1\",\n",
    "    \"fare_amount\": 44,\n",
    "    \"extra\": 0,\n",
    "    \"mta_tax\": 0.5,\n",
    "    \"tolls_amount\": 0,\n",
    "    \"imp_surcharge\": 0.3,\n",
    "    \"total_amount\": 53.76,\n",
    "    \"pickup_location_id\": \"138\",\n",
    "    \"dropoff_location_id\": \"68\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID=\"3222589416174256128\"\n",
    "INPUT_DATA_FILE=\"taxi-pred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      7.8573332053074241\n",
      "    ]\n",
      "  ],\n",
      "  \"deployedModelId\": \"5440990464654180352\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl \\\n",
    "-X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/endpoints/$ENDPOINT_ID:predict \\\n",
    "-d \"@taxi-pred.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict at BQ for Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "Predict_explain_query=f\"\"\"\"\"\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "query_job = client.query(EXPLAIN_QUERY)\n",
    "print(query_job.result().to_dataframe())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
